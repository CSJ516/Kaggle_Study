{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-[Chapter-08]-텍스트-분석\" data-toc-modified-id=\"1.-[Chapter-08]-텍스트-분석-1\">1. [Chapter 08] 텍스트 분석</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-텍스트-분석-이해\" data-toc-modified-id=\"1.1-텍스트-분석-이해-1.1\">1.1 텍스트 분석 이해</a></span></li><li><span><a href=\"#1.2-텍스트-사전-준비-작업(텍스트-전처리)---텍스트-정규화\" data-toc-modified-id=\"1.2-텍스트-사전-준비-작업(텍스트-전처리)---텍스트-정규화-1.2\">1.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1-텍스트-토큰화\" data-toc-modified-id=\"1.2.1-텍스트-토큰화-1.2.1\">1.2.1 텍스트 토큰화</a></span></li><li><span><a href=\"#1.2.2-스톱-워드-제거\" data-toc-modified-id=\"1.2.2-스톱-워드-제거-1.2.2\">1.2.2 스톱 워드 제거</a></span></li><li><span><a href=\"#1.2.3-Stemming과-Lemmatization\" data-toc-modified-id=\"1.2.3-Stemming과-Lemmatization-1.2.3\">1.2.3 Stemming과 Lemmatization</a></span></li></ul></li><li><span><a href=\"#1.3-Bag-of-Words---BOW\" data-toc-modified-id=\"1.3-Bag-of-Words---BOW-1.3\">1.3 Bag of Words - BOW</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.3.1-BOW-피처-벡터화\" data-toc-modified-id=\"1.3.1-BOW-피처-벡터화-1.3.1\">1.3.1 BOW 피처 벡터화</a></span></li><li><span><a href=\"#1.3.2-사이킷런의-Count-및-TF-IDF-벡터화-구현\" data-toc-modified-id=\"1.3.2-사이킷런의-Count-및-TF-IDF-벡터화-구현-1.3.2\">1.3.2 사이킷런의 Count 및 TF-IDF 벡터화 구현</a></span></li><li><span><a href=\"#1.3.3-BOW-벡터화를-위한-희소-행렬\" data-toc-modified-id=\"1.3.3-BOW-벡터화를-위한-희소-행렬-1.3.3\">1.3.3 BOW 벡터화를 위한 희소 행렬</a></span></li><li><span><a href=\"#1.3.4-희소-행렬---COO-형식\" data-toc-modified-id=\"1.3.4-희소-행렬---COO-형식-1.3.4\">1.3.4 희소 행렬 - COO 형식</a></span></li><li><span><a href=\"#1.3.5-희소-행렬---CSR-형식\" data-toc-modified-id=\"1.3.5-희소-행렬---CSR-형식-1.3.5\">1.3.5 희소 행렬 - CSR 형식</a></span></li></ul></li><li><span><a href=\"#1.4-텍스트-분류-실습---20-뉴스그룹-분류\" data-toc-modified-id=\"1.4-텍스트-분류-실습---20-뉴스그룹-분류-1.4\">1.4 텍스트 분류 실습 - 20 뉴스그룹 분류</a></span></li><li><span><a href=\"#1.5-감성-분석\" data-toc-modified-id=\"1.5-감성-분석-1.5\">1.5 감성 분석</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Chapter 08] 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 텍스트 분석 이해\n",
    "\n",
    "- 텍스트 분류 : 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법으로, 지도학습을 적용 (ex) 스팸 메일 검출)\n",
    "\n",
    "\n",
    "- 감성 분석 : 텍스트에서 나타나는 감정/판단/믿음/의견 등의 주관적 요소를 분석하는 기법으로, 지도학습과 비지도학습 모두 적용 가능\n",
    "\n",
    "\n",
    "- 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법 (ex) 토픽 모델링)\n",
    "\n",
    "\n",
    "- 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화를 수행하는 기법, 문서들간의 유사도를 측정해 비슷한 문서끼리 모을 수 있는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트 분석 수행 프로세스**\n",
    "\n",
    "1) 텍스트 전처리 : 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미 없는 단어 제거, 어근 추출 등의 텍스트 정규화 작업\n",
    "\n",
    "2) 피처 벡터화/추출 : 텍스트에서 피처를 추출하고 여기에 특정 의미를 가지는 숫자형 값인 벡터 값 할당 (ex) BOW, Word2Vec)\n",
    "\n",
    "3) ML 모델 수립 및 학습/예측/평가 : ML 모델을 적용해 학습/예측 및 평가 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화\n",
    "\n",
    "- 클렌징 : HTML 태그나 특정 기호 등 불필요한 문자, 기호를 사전에 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 텍스트 토큰화\n",
    "\n",
    "- 문장 토큰화 : 문장의 마지막을 뜻하는 기호(. \\n 등)에 따라 분리. 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 때 사용\n",
    "\n",
    "\n",
    "**sent_tokenize()**를 이용해 문장 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.\\\n",
    "               You can see it out your window or on your television.\\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 토큰화 : 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리. BOW와 같이 단어의 순서가 중요하지 않은 경우 단어 토큰화만 사용해도 충분\n",
    "\n",
    "**word_tokenize()**를 이용해 단어 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 토큰화 수행 시 문맥적인 의미가 무시되는 문제를 해결하고자 도입된 것이 n-gram\n",
    "\n",
    "n-gram은 연속적으로 2개의 단어들을 순차적으로 이동하면서 단어들을 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 스톱 워드 제거\n",
    "\n",
    "- 스톱 워드 제거 : 분석에 큰 의미가 없는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거\n",
    "    for word in sentence:\n",
    "        word = word.lower()   # 소문자 변환\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Stemming과 Lemmatization\n",
    "\n",
    "- Stemming : 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 것\n",
    "\n",
    "**LancasterStemmer()**를 이용해 Stemming 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work의 경우 단순한 변화이므로 원형 단어를 제대로 인식하지만, amuse와 happy는 정확한 원형을 찾지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾는 것 (시간이 더 오래 걸림)\n",
    "\n",
    "**WordNetLemmatizer()**를 이용해 Lemmatization를 수행하며, 단어의 품사(v, a)를 같이 입력해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bag of Words - BOW\n",
    "\n",
    "- BOW : 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
    "\n",
    "장점 : 쉽고 빠르며, 예상보다 문서의 특징을 잘 나타내어 여러 분야에서 활용도가 높음\n",
    "\n",
    "단점 : 문맥 의미 반영이 부족하고, 희소 행렬 문제가 있음\n",
    "\n",
    "cf) 희소 행렬 : 대규모의 칼럼으로 구성된 행렬에서 대부분의 값이 0으로 채워지는 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 BOW 피처 벡터화\n",
    "\n",
    "- 카운트 벡터화 : 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수를 부여하는 경우. 값이 높을수록 중요한 단어로 인식\n",
    "\n",
    "\n",
    "- TF-IDF : 기본적으로 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 자주 나타나는 범용적인 단어에는 페널티를 주는 방식\n",
    "\n",
    "**문서마다 텍스트가 길고 문서의 개수가 많은 경우 TF-IDF 방식을 사용하는 것이 더 좋은 예측 성능을 보장**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 사이킷런의 Count 및 TF-IDF 벡터화 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CountVectorizer()** 의 입력 파라미터\n",
    "\n",
    "    max_df : 전체 문서에 걸쳐 너무 높은 빈도수를 가지는 단어 피처 제외\n",
    "    \n",
    "    min_df : 전체 문서에 걸쳐 너무 낮은 빈도수를 가지는 단어 피처 제외\n",
    "    \n",
    "    max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "    \n",
    "    stop_words : english로 지정하면 영어의 스톱 워드 제외\n",
    "    \n",
    "    n_gram_range : BOW 모델의 단어 순서를 보강하기 위한 n_gram 범위 설정 (범위 최솟값, 범위 최댓값)\n",
    "    \n",
    "    analyzer, token_pattern, tokenizer\n",
    "    \n",
    "    \n",
    "- CountVectorizer()를 이용한 피처 벡터화\n",
    "\n",
    "    1) 사전 데이터 가공 : 모든 문자를 소문자로 변환하는 등 전처리 작업 수행\n",
    "\n",
    "    2) 토큰화 : Default는 단어 기준(analyzer = True)이며, n_gram_range를 반영하여 토큰화 수행\n",
    "\n",
    "    3) 텍스트 정규화 : stop words 필터링 수행 (Stemmer, Lemmatize는 자체 지원 X)\n",
    "\n",
    "    4) 피처 벡터화 : max_df, min_df, max_features 등의 파라미터로 Token된 단어들을 feature extraction 후 vectorization 적용\n",
    "    \n",
    "\n",
    "- **TfidfVectorizer()**의 파라미터와 변환 방법은 CountVectorizer()와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 BOW 벡터화를 위한 희소 행렬\n",
    "\n",
    "희소 행렬은 너무 많은 불필요한 0 값으로 인해 메모리 공간이 많이 필요하며, 연산 시에도 시간이 많이 소모됨\n",
    "\n",
    "COO 형식과 CSR 형식으로 적은 메모리 공간을 차지할 수 있도록 변환할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 희소 행렬 - COO 형식\n",
    "\n",
    "- COO 형식(Coordinate 좌표) : 0이 아닌 데이터와 그 데이터가 가리키는 행, 열의 위치를 별도의 배열로 저장하는 방식\n",
    "\n",
    "**사이파이의 sparse.coo_matrix**를 이용해 희소 행렬 전환을 COO 형식으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row = np.array([0,0,1])\n",
    "col = np.array([0,2,1])\n",
    "\n",
    "# coo_matrix를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row, col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 희소 행렬 - CSR 형식\n",
    "\n",
    "- CSR 형식(Compressed Sparse Row) : COO 형식이 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 것으로, 행 위치 배열 내에 있는 고유한 값의 시작 위치만 다시 별도의 위치 배열로 가지는 변환 방식\n",
    "\n",
    "**사이파이의 sparse.csr_matrix**를 이용해 CSR 방식의 변환 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row, col)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col, row_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 사용 시에는 다음과 같이 밀집 행렬을 입력하여 COO, CSR 희소 행렬 생성\n",
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 텍스트 분류 실습 - 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
