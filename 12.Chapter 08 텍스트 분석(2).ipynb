{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-[Chapter-08]-텍스트-분석\" data-toc-modified-id=\"1.-[Chapter-08]-텍스트-분석-1\">1. [Chapter 08] 텍스트 분석</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.6-토픽-모델링---20-뉴스그룹\" data-toc-modified-id=\"1.6-토픽-모델링---20-뉴스그룹-1.1\">1.6 토픽 모델링 - 20 뉴스그룹</a></span></li><li><span><a href=\"#1.7-문서-군집화-소개와-실습\" data-toc-modified-id=\"1.7-문서-군집화-소개와-실습-1.2\">1.7 문서 군집화 소개와 실습</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.7.1-Opinion-Review-데이터-세트를-이용한-문서-군집화-수행하기\" data-toc-modified-id=\"1.7.1-Opinion-Review-데이터-세트를-이용한-문서-군집화-수행하기-1.2.1\">1.7.1 Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Chapter 08] 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 토픽 모델링 - 20 뉴스그룹\n",
    "\n",
    "- 토픽 모델링(Topic Modeling) : 문서 집합에 숨어 있는 주체를 찾아내는 것 (ex) LSA, LDA)\n",
    "\n",
    "**LatentDirichletAllocation** 클래스로 LDA 기반의 토픽 모델링 수행. **LDA는 Count 기반의 벡터화만 사용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape:  (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제 추출\n",
    "cats = ['rec.motorcycles','rec.sport.baseball','comp.graphics','comp.windows.x','talk.politics.mideast',\\\n",
    "        'soc.religion.christian','sci.electronics','sci.med']\n",
    "\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'), categories=cats, random_state=0)\n",
    "\n",
    "count_vec = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
    "feat_vec = count_vec.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape: ', feat_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.46251560e+02, 1.18842248e+02, 1.51715288e+02, ...,\n",
       "        1.00147234e+02, 7.63673375e+01, 1.17028758e+02],\n",
       "       [1.25033020e-01, 1.25052288e-01, 1.25003012e-01, ...,\n",
       "        1.10644583e+02, 1.51405141e-01, 5.09788954e+01],\n",
       "       [1.25103419e-01, 1.25075224e-01, 1.25082214e-01, ...,\n",
       "        6.72008817e+01, 1.25138615e-01, 2.48516614e+00],\n",
       "       ...,\n",
       "       [1.05055615e+02, 4.94858011e-01, 2.52075927e+01, ...,\n",
       "        1.80695744e+01, 1.25115936e-01, 8.33321314e+00],\n",
       "       [1.25147502e-01, 2.27058083e+02, 5.45176328e+00, ...,\n",
       "        1.41751120e+00, 7.67217701e+01, 4.49861794e+01],\n",
       "       [1.25096012e-01, 4.05666840e+00, 1.25049904e-01, ...,\n",
       "        1.63821915e+02, 1.25049991e-01, 1.49550227e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)   # n_components로 토픽 개수 조정\n",
    "lda.fit(feat_vec)\n",
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "components_는 개별 토픽별로 각 word 피처가 얼마나 많이 할당됐는지에 대한 수치를 의미\n",
    "\n",
    "높은 값일수록 해당 word 피처는 그 토픽의 중심 word가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "year said don didn know game just time went people think did like say home\n",
      "Topic # 1\n",
      "god people jesus church think believe christ say does don christian know christians bible faith\n",
      "Topic # 2\n",
      "know does thanks like question information help time post advance book just looking group read\n",
      "Topic # 3\n",
      "edu com graphics mail ftp information available data pub list computer send software ca 3d\n",
      "Topic # 4\n",
      "israel jews jewish israeli dos dos arab turkish people war turkey dos state government greek history\n",
      "Topic # 5\n",
      "file image use program window jpeg windows display version color server files using available motif\n",
      "Topic # 6\n",
      "armenian armenians people health medical armenia disease turkish patients cancer russian 10 azerbaijan children 92\n",
      "Topic # 7\n",
      "like just don ve use good think time know way make used bike want need\n"
     ]
    }
   ],
   "source": [
    "# 각 토픽별로 연관도가 높은 순으로 word를 나열하는 함수\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_idx)\n",
    "        \n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array 인덱스를 반환\n",
    "        topic_word_idx = topic.argsort()[::-1]\n",
    "        top_idx = topic_word_idx[:no_top_words]\n",
    "        \n",
    "        # top_idx 대상인 인덱스별로 feature_names에 해당하는 word feature 추출\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_idx])\n",
    "        print(feature_concat)\n",
    "        \n",
    "# CountVectorizer 객체 내의 전체 word의 명칭 추출\n",
    "feature_names = count_vec.get_feature_names()\n",
    "\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명확한 주제어가 추출되지 않고 일반적인 단어가 주를 이루는 Topic 들도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 문서 군집화 소개와 실습\n",
    "\n",
    "- 문서 군집화 : 비슷한 텍스트 구성의 문서를 군집화하는 것. 텍스트 분류 기반의 문서 분류와 달리 비지도학습 기반으로 동작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Opinosis+Opinion+%26frasl%3B+Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  \n",
       "0                                                ...  \n",
       "1                                                ...  \n",
       "2                                                ...  \n",
       "3                                                ...  \n",
       "4                                                ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "path = r'C:\\Users\\dalgo\\06.Kaggle_Study\\data\\topics'\n",
    "all_files = glob.glob(os.path.join(path, '*.data'))\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
    "    \n",
    "    # 절대 경로로 주어진 파일명을 가공하고, 맨 마지막 .data 확장자 제거\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    \n",
    "    # 파일명 list와 파일 내용 list에 파일명과 파일 내용 추가\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "    \n",
    "# 파일명 list와 파일 내용 list 객체를 df로 생성\n",
    "document_df = pd.DataFrame({'filename': filename_list, 'opinion_text': opinion_text})\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "def ngrams(string, n=2):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 TF-IDF 형태로 피처 벡터화\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english',\\\n",
    "                            ngram_range=(1,2), min_df=0.05, max_df=0.85)\n",
    "feature_vec = tfidf_vec.fit_transform(document_df['opinion_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
