{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-[Chapter-08]-텍스트-분석\" data-toc-modified-id=\"1.-[Chapter-08]-텍스트-분석-1\">1. [Chapter 08] 텍스트 분석</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-텍스트-분석-이해\" data-toc-modified-id=\"1.1-텍스트-분석-이해-1.1\">1.1 텍스트 분석 이해</a></span></li><li><span><a href=\"#1.2-텍스트-사전-준비-작업(텍스트-전처리)---텍스트-정규화\" data-toc-modified-id=\"1.2-텍스트-사전-준비-작업(텍스트-전처리)---텍스트-정규화-1.2\">1.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1-텍스트-토큰화\" data-toc-modified-id=\"1.2.1-텍스트-토큰화-1.2.1\">1.2.1 텍스트 토큰화</a></span></li><li><span><a href=\"#1.2.2-스톱-워드-제거\" data-toc-modified-id=\"1.2.2-스톱-워드-제거-1.2.2\">1.2.2 스톱 워드 제거</a></span></li><li><span><a href=\"#1.2.3-Stemming과-Lemmatization\" data-toc-modified-id=\"1.2.3-Stemming과-Lemmatization-1.2.3\">1.2.3 Stemming과 Lemmatization</a></span></li></ul></li><li><span><a href=\"#1.3-Bag-of-Words---BOW\" data-toc-modified-id=\"1.3-Bag-of-Words---BOW-1.3\">1.3 Bag of Words - BOW</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.3.1-BOW-피처-벡터화\" data-toc-modified-id=\"1.3.1-BOW-피처-벡터화-1.3.1\">1.3.1 BOW 피처 벡터화</a></span></li><li><span><a href=\"#1.3.2-사이킷런의-Count-및-TF-IDF-벡터화-구현\" data-toc-modified-id=\"1.3.2-사이킷런의-Count-및-TF-IDF-벡터화-구현-1.3.2\">1.3.2 사이킷런의 Count 및 TF-IDF 벡터화 구현</a></span></li><li><span><a href=\"#1.3.3-BOW-벡터화를-위한-희소-행렬\" data-toc-modified-id=\"1.3.3-BOW-벡터화를-위한-희소-행렬-1.3.3\">1.3.3 BOW 벡터화를 위한 희소 행렬</a></span></li><li><span><a href=\"#1.3.4-희소-행렬---COO-형식\" data-toc-modified-id=\"1.3.4-희소-행렬---COO-형식-1.3.4\">1.3.4 희소 행렬 - COO 형식</a></span></li><li><span><a href=\"#1.3.5-희소-행렬---CSR-형식\" data-toc-modified-id=\"1.3.5-희소-행렬---CSR-형식-1.3.5\">1.3.5 희소 행렬 - CSR 형식</a></span></li></ul></li><li><span><a href=\"#1.4-텍스트-분류-실습---20-뉴스그룹-분류\" data-toc-modified-id=\"1.4-텍스트-분류-실습---20-뉴스그룹-분류-1.4\">1.4 텍스트 분류 실습 - 20 뉴스그룹 분류</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.4.1-텍스트-정규화\" data-toc-modified-id=\"1.4.1-텍스트-정규화-1.4.1\">1.4.1 텍스트 정규화</a></span></li><li><span><a href=\"#1.4.2-피처-벡터화-변환과-머신러닝-모델-학습/예측/평가\" data-toc-modified-id=\"1.4.2-피처-벡터화-변환과-머신러닝-모델-학습/예측/평가-1.4.2\">1.4.2 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가</a></span></li><li><span><a href=\"#1.4.3-사이킷런-파이프라인-사용-및-GridSearchCV와의-결합\" data-toc-modified-id=\"1.4.3-사이킷런-파이프라인-사용-및-GridSearchCV와의-결합-1.4.3\">1.4.3 사이킷런 파이프라인 사용 및 GridSearchCV와의 결합</a></span></li></ul></li><li><span><a href=\"#1.5-감성-분석\" data-toc-modified-id=\"1.5-감성-분석-1.5\">1.5 감성 분석</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.5.1-지도학습-기반-감성-분석-실습---IMDB-영화평\" data-toc-modified-id=\"1.5.1-지도학습-기반-감성-분석-실습---IMDB-영화평-1.5.1\">1.5.1 지도학습 기반 감성 분석 실습 - IMDB 영화평</a></span></li><li><span><a href=\"#1.5.2-비지도학습-기반-감성-분석-소개\" data-toc-modified-id=\"1.5.2-비지도학습-기반-감성-분석-소개-1.5.2\">1.5.2 비지도학습 기반 감성 분석 소개</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Chapter 08] 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 텍스트 분석 이해\n",
    "\n",
    "- 텍스트 분류 : 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법으로, 지도학습을 적용 (ex) 스팸 메일 검출)\n",
    "\n",
    "\n",
    "- 감성 분석 : 텍스트에서 나타나는 감정/판단/믿음/의견 등의 주관적 요소를 분석하는 기법으로, 지도학습과 비지도학습 모두 적용 가능\n",
    "\n",
    "\n",
    "- 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법 (ex) 토픽 모델링)\n",
    "\n",
    "\n",
    "- 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화를 수행하는 기법, 문서들간의 유사도를 측정해 비슷한 문서끼리 모을 수 있는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트 분석 수행 프로세스**\n",
    "\n",
    "1) 텍스트 전처리 : 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미 없는 단어 제거, 어근 추출 등의 텍스트 정규화 작업\n",
    "\n",
    "2) 피처 벡터화/추출 : 텍스트에서 피처를 추출하고 여기에 특정 의미를 가지는 숫자형 값인 벡터 값 할당 (ex) BOW, Word2Vec)\n",
    "\n",
    "3) ML 모델 수립 및 학습/예측/평가 : ML 모델을 적용해 학습/예측 및 평가 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화\n",
    "\n",
    "- 클렌징 : HTML 태그나 특정 기호 등 불필요한 문자, 기호를 사전에 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 텍스트 토큰화\n",
    "\n",
    "- 문장 토큰화 : 문장의 마지막을 뜻하는 기호(. \\n 등)에 따라 분리. 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 때 사용\n",
    "\n",
    "\n",
    "**sent_tokenize()**를 이용해 문장 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.\\\n",
    "               You can see it out your window or on your television.\\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 토큰화 : 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리. BOW와 같이 단어의 순서가 중요하지 않은 경우 단어 토큰화만 사용해도 충분\n",
    "\n",
    "**word_tokenize()**를 이용해 단어 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 토큰화 수행 시 문맥적인 의미가 무시되는 문제를 해결하고자 도입된 것이 n-gram\n",
    "\n",
    "n-gram은 연속적으로 2개의 단어들을 순차적으로 이동하면서 단어들을 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 스톱 워드 제거\n",
    "\n",
    "- 스톱 워드 제거 : 분석에 큰 의미가 없는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거\n",
    "    for word in sentence:\n",
    "        word = word.lower()   # 소문자 변환\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Stemming과 Lemmatization\n",
    "\n",
    "- Stemming : 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 것\n",
    "\n",
    "**LancasterStemmer()**를 이용해 Stemming 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work의 경우 단순한 변화이므로 원형 단어를 제대로 인식하지만, amuse와 happy는 정확한 원형을 찾지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾는 것 (시간이 더 오래 걸림)\n",
    "\n",
    "**WordNetLemmatizer()**를 이용해 Lemmatization를 수행하며, 단어의 품사(v, a)를 같이 입력해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bag of Words - BOW\n",
    "\n",
    "- BOW : 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
    "\n",
    "장점 : 쉽고 빠르며, 예상보다 문서의 특징을 잘 나타내어 여러 분야에서 활용도가 높음\n",
    "\n",
    "단점 : 문맥 의미 반영이 부족하고, 희소 행렬 문제가 있음\n",
    "\n",
    "cf) 희소 행렬 : 대규모의 칼럼으로 구성된 행렬에서 대부분의 값이 0으로 채워지는 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 BOW 피처 벡터화\n",
    "\n",
    "- 카운트 벡터화 : 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수를 부여하는 경우. 값이 높을수록 중요한 단어로 인식\n",
    "\n",
    "\n",
    "- TF-IDF : 기본적으로 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 자주 나타나는 범용적인 단어에는 페널티를 주는 방식\n",
    "\n",
    "**문서마다 텍스트가 길고 문서의 개수가 많은 경우 TF-IDF 방식을 사용하는 것이 더 좋은 예측 성능을 보장**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 사이킷런의 Count 및 TF-IDF 벡터화 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CountVectorizer()** 의 입력 파라미터\n",
    "\n",
    "    max_df : 전체 문서에 걸쳐 너무 높은 빈도수를 가지는 단어 피처 제외\n",
    "    \n",
    "    min_df : 전체 문서에 걸쳐 너무 낮은 빈도수를 가지는 단어 피처 제외\n",
    "    \n",
    "    max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "    \n",
    "    stop_words : english로 지정하면 영어의 스톱 워드 제외\n",
    "    \n",
    "    n_gram_range : BOW 모델의 단어 순서를 보강하기 위한 n_gram 범위 설정 (범위 최솟값, 범위 최댓값)\n",
    "    \n",
    "    analyzer, token_pattern, tokenizer\n",
    "    \n",
    "    \n",
    "- CountVectorizer()를 이용한 피처 벡터화\n",
    "\n",
    "    1) 사전 데이터 가공 : 모든 문자를 소문자로 변환하는 등 전처리 작업 수행\n",
    "\n",
    "    2) 토큰화 : Default는 단어 기준(analyzer = True)이며, n_gram_range를 반영하여 토큰화 수행\n",
    "\n",
    "    3) 텍스트 정규화 : stop words 필터링 수행 (Stemmer, Lemmatize는 자체 지원 X)\n",
    "\n",
    "    4) 피처 벡터화 : max_df, min_df, max_features 등의 파라미터로 Token된 단어들을 feature extraction 후 vectorization 적용\n",
    "    \n",
    "\n",
    "- **TfidfVectorizer()**의 파라미터와 변환 방법은 CountVectorizer()와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 BOW 벡터화를 위한 희소 행렬\n",
    "\n",
    "희소 행렬은 너무 많은 불필요한 0 값으로 인해 메모리 공간이 많이 필요하며, 연산 시에도 시간이 많이 소모됨\n",
    "\n",
    "COO 형식과 CSR 형식으로 적은 메모리 공간을 차지할 수 있도록 변환할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 희소 행렬 - COO 형식\n",
    "\n",
    "- COO 형식(Coordinate 좌표) : 0이 아닌 데이터와 그 데이터가 가리키는 행, 열의 위치를 별도의 배열로 저장하는 방식\n",
    "\n",
    "**사이파이의 sparse.coo_matrix**를 이용해 희소 행렬 전환을 COO 형식으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row = np.array([0,0,1])\n",
    "col = np.array([0,2,1])\n",
    "\n",
    "# coo_matrix를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row, col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 희소 행렬 - CSR 형식\n",
    "\n",
    "- CSR 형식(Compressed Sparse Row) : COO 형식이 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 것으로, 행 위치 배열 내에 있는 고유한 값의 시작 위치만 다시 별도의 위치 배열로 가지는 변환 방식\n",
    "\n",
    "**사이파이의 sparse.csr_matrix**를 이용해 CSR 방식의 변환 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row, col)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col, row_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 사용 시에는 다음과 같이 밀집 행렬을 입력하여 COO, CSR 희소 행렬 생성\n",
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 텍스트 분류 실습 - 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 텍스트 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도 \n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도 \\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('target 클래스의 이름들 \\n', news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순수한 내용으로만 분류를 수행하고자 제목, 소속, 이메일 등의 다른 정보는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 11314, 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "# subset='train'으로 학습용 데이터만 추출, remove로 내용만 추출\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "# subset='test'으로 테스트 데이터만 추출, remove로 내용만 추출\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers','footers','quotes'), random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print('학습 데이터 크기 {0}, 테스트 데이터 크기 {1}'.format(len(train_news.data), len(test_news.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가\n",
    "\n",
    "**반드시 학습 데이터를 이용해 fit()이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 변환**해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 CountVectorizer Shape:  (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 피처 벡터화 변환 수행\n",
    "cnt_vec = CountVectorizer()\n",
    "X_train_cnt_vec = cnt_vec.fit_transform(X_train)\n",
    "X_test_cnt_vec = cnt_vec.transform(X_test)\n",
    "\n",
    "print('학습 데이터 텍스트의 CountVectorizer Shape: ', X_train_cnt_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도: 0.607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vec, y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vec)\n",
    "print('CountVectorized Logistic Regression의 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression의 예측 정확도: 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train_tf_vec = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tf_vec = tfidf_vec.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tf_vec, y_train)\n",
    "pred = lr_clf.predict(X_test_tf_vec)\n",
    "print('TF-IDF Logistic Regression의 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 좋은 예측 결과를 나타낸 TF-IDF 벡터화에 다양한 파라미터 적용\n",
    "\n",
    "GridsearchCV를 이용해 로지스틱 회귀의 하이퍼 파라미터 최적화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 63.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best C parameter:  {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression의 예측 정확도: 0.701\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링을 추가하고 ngram을 (1,2)로 변경\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)\n",
    "X_train_tf_vec = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tf_vec = tfidf_vec.transform(X_test)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# 최적 C 값 도출 튜닝 수행. CV는 3폴드 세트\n",
    "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(X_train_tf_vec, y_train)\n",
    "print('Logistic Regression best C parameter: ', grid_cv.best_params_)\n",
    "\n",
    "# 최적 C 값으로 학습된 grid_cv로 예측 및 정확도 평가\n",
    "pred = grid_cv.predict(X_test_tf_vec)\n",
    "print('TF-IDF Vectorized Logistic Regression의 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 사이킷런 파이프라인 사용 및 GridSearchCV와의 결합\n",
    "\n",
    "파이프라인을 사용하면 더 직관적인 ML 모델 코드를 생성할 수 있으며, 수행 시간을 절약할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 Logistic Regression의 예측 정확도: 0.701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vec', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(C=10))\n",
    "])\n",
    "\n",
    "# pipeline의 fit()과 predict()만으로 한꺼번에 피처 벡터화와 ML 학습/예측 가능\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression의 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('tfidf_vec', TfidfVectorizer(stop_words='english')),\n",
    "#     ('lr_clf', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# # 각각의 객체 변수에 _ 2개를 연달아 붙여 파라미터 이름과 값을 설정\n",
    "# params = {'tfidf_vec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "#           'tfidf_vec__max_df': [100, 300, 700],\n",
    "#           'lr_clf__C': [1,5,10]}\n",
    "\n",
    "# grid_cv = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "# grid_cv.fit(X_train, y_train)\n",
    "# print(grid_cv.best_params_, grid_cv.best_score_)\n",
    "\n",
    "# pred = grid_cv.predict(X_test)\n",
    "# print('Pipeline을 통한 Logistic Regression의 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 감성 분석\n",
    "\n",
    "- 감성 분석 : 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기반으로 감성 수치를 계산\n",
    "\n",
    "\n",
    "지도학습은 일반적인 텍스트 기반의 분류와 거의 동일하며, 비지도학습은 'Lexicon'이라는 일종의 감성 어휘 사전을 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 지도학습 기반 감성 분석 실습 - IMDB 영화평\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data 의 labeledTrainData.tsv 파일 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_csv('./data/labeledTrainData.tsv', header=0, sep='\\t', quoting=3)\n",
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment 결과 값이 1이면 긍정적 평가, 0이면 부정적 평가를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# <br> html 태그는 replace 함수로 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />', ' ')\n",
    "\n",
    "# 정규표현식을 이용해 영어 문자열이 아닌 문자는 모두 공백으로 변환\n",
    "review_df['review'] = review_df['review'].apply(lambda x : re.sub('[^a-zA-Z]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id', 'sentiment'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 벡터화, LogisticRegression, 그리고 예측 성능 평가는 이진 분류임을 고려해 정확도와 ROC-AUC 모두 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.8936, ROC-AUC: 0.9598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vec', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "print('예측 정확도: {0:.4f}, ROC-AUC: {1:.4f}'.format(accuracy_score(y_test, pred), roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 비지도학습 기반 감성 분석 소개\n",
    "\n",
    "- **SentiWordNet** : NLTK 패키지의 WordNet과 유사하게 Synset 개념을 감성 분석에 적용한 것. 예측 정확도가 높지 않아 잘 사용하지는 않음\n",
    "\n",
    "1) 문서를 문장 단위로 분해\n",
    "\n",
    "2) 다시 문장을 단어 단위로 토큰화하고 품사 태깅\n",
    "\n",
    "3) 품사 태깅된 단어 기반으로 synset 객체와 senti_synset 객체 생성\n",
    "\n",
    "4) Senti_synset에서 긍정 감성/부정 감성 지수를 구하고 이를 모두 합산해 특정 임계치 값 이상일 때 긍정 감성, 그렇지 않을 때 부정 감성으로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# 간단한 NLTK PennTreebank Tag를 기반으로 WordNet 기반의 품사 Tag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "def swn_polarity(text):\n",
    "    # 감성 지수 초기화\n",
    "    sentiment = 0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분해된 문장별로 단어 토큰 -> 품사 태깅 후에 SentiSynset 생성 -> 감성 지수 합산\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # NLTK 기반의 품사 태깅 문장 추출\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        for word, tag in tagged_sentence:\n",
    "            # WordNet 기반 품사 태깅과 어근 추출\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체 생성\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
    "            # 모든 단어에 대해 긍정 감성 지수는 +, 부정 감성 지수는 -로 합산해 감성 지수 계산\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())\n",
    "            tokens_count += 1\n",
    "            \n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    \n",
    "    # 총 score가 0 이상일 경우 긍정 1, 그렇지 않을 경우 부정 0\n",
    "    if sentiment >= 0:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentiWordNet을 이용한 IMDB 영화 감상평 감성 분석\n",
    "review_df['preds'] = review_df['review'].apply(lambda x : swn_polarity(x))\n",
    "y_target = review_df['sentiment'].values\n",
    "preds = review_df['preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7668 4832]\n",
      " [3636 8864]]\n",
      "정확도:  0.6613\n",
      "정밀도:  0.6472\n",
      "재현율:  0.7091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(confusion_matrix(y_target, preds))\n",
    "print('정확도: ', np.round(accuracy_score(y_target, preds), 4))\n",
    "print('정밀도: ', np.round(precision_score(y_target, preds), 4))\n",
    "print('재현율: ', np.round(recall_score(y_target, preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **VADER** : 주로 소셜 미디어의 텍스트에 대한 감성 분석을 제공. 뛰어난 결과를 제공하며, 비교적 빠른 수행 시간으로 대용량 텍스트 데이터에 자주 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.119, 'neu': 0.755, 'pos': 0.126, 'compound': -0.0678}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "senti_scores = senti_analyzer.polarity_scores(review_df['review'][0])\n",
    "print(senti_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neg는 부정 감성 지수, neu는 중립 감성 지수, pos는 긍정 감성 지수, compound는 -1에서 1 사이의 감성 지수를 표현한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6729  5771]\n",
      " [ 1858 10642]]\n",
      "정확도:  0.6948\n",
      "정밀도:  0.6484\n",
      "재현율:  0.8514\n"
     ]
    }
   ],
   "source": [
    "def vader_polarity(review, threshold=0.1):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # compound 값에 기반해 threshold 입력값보다 크면 1, 그렇지 않으면 0\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "# 레코드별로 vader_polarity()를 수행하고 결과를 vader_preds에 저장\n",
    "review_df['vader_preds'] = review_df['review'].apply(lambda x : vader_polarity(x, 0.1))\n",
    "y_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values\n",
    "\n",
    "print(confusion_matrix(y_target, vader_preds))\n",
    "print('정확도: ', np.round(accuracy_score(y_target, vader_preds), 4))\n",
    "print('정밀도: ', np.round(precision_score(y_target, vader_preds), 4))\n",
    "print('재현율: ', np.round(recall_score(y_target, vader_preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pattern** : 예측 성능 측면에서 가장 주목받고 있으나, 파이썬 3 버전에서 완벽하게 지원하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
