{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-[Chapter-08]-텍스트-분석\" data-toc-modified-id=\"1.-[Chapter-08]-텍스트-분석-1\">1. [Chapter 08] 텍스트 분석</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-텍스트-분석-이해\" data-toc-modified-id=\"1.1-텍스트-분석-이해-1.1\">1.1 텍스트 분석 이해</a></span></li><li><span><a href=\"#1.2-텍스트-사전-준비-작업(텍스트-전처리)---텍스트-정규화\" data-toc-modified-id=\"1.2-텍스트-사전-준비-작업(텍스트-전처리)---텍스트-정규화-1.2\">1.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화</a></span></li><li><span><a href=\"#1.3-Bag-of-Words---BOW\" data-toc-modified-id=\"1.3-Bag-of-Words---BOW-1.3\">1.3 Bag of Words - BOW</a></span></li><li><span><a href=\"#1.4-텍스트-분류-실습---20-뉴스그룹-분류\" data-toc-modified-id=\"1.4-텍스트-분류-실습---20-뉴스그룹-분류-1.4\">1.4 텍스트 분류 실습 - 20 뉴스그룹 분류</a></span></li><li><span><a href=\"#1.5-감성-분석\" data-toc-modified-id=\"1.5-감성-분석-1.5\">1.5 감성 분석</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Chapter 08] 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 텍스트 분석 이해\n",
    "\n",
    "- 텍스트 분류 : 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법으로, 지도학습을 적용 (ex) 스팸 메일 검출)\n",
    "\n",
    "\n",
    "- 감성 분석 : 텍스트에서 나타나는 감정/판단/믿음/의견 등의 주관적 요소를 분석하는 기법으로, 지도학습과 비지도학습 모두 적용 가능\n",
    "\n",
    "\n",
    "- 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법 (ex) 토픽 모델링)\n",
    "\n",
    "\n",
    "- 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화를 수행하는 기법, 문서들간의 유사도를 측정해 비슷한 문서끼리 모을 수 있는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트 분석 수행 프로세스**\n",
    "\n",
    "1) 텍스트 전처리 : 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미 없는 단어 제거, 어근 추출 등의 텍스트 정규화 작업\n",
    "\n",
    "2) 피처 벡터화/추출 : 텍스트에서 피처를 추출하고 여기에 벡터 값 할당 (ex) BOW(Count 기반, TF-IDF 기반), Word2Vec)\n",
    "\n",
    "3) ML 모델 수립 및 학습/예측/평가 : ML 모델을 적용해 학습/예측 및 평가 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화\n",
    "\n",
    "- 클렌징 : HTML 태그나 특정 기호 등 불필요한 문자, 기호를 사전에 제거\n",
    "\n",
    "\n",
    "- 문장 토큰화 : 문장의 마지막을 뜻하는 기호(. \\n 등)에 따라 분리. 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 때 사용\n",
    "\n",
    "\n",
    "**sent_tokenize()**를 이용해 문장 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dalgo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.\\\n",
    "               You can see it out your window or on your television.\\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 토큰화 : 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리. BOW와 같이 단어의 순서가 중요하지 않은 경우 단어 토큰화만 사용해도 충분\n",
    "\n",
    "**word_tokenize()**를 이용해 단어 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 토큰화 수행 시 문맥적인 의미가 무시되는 문제를 해결하고자 도입된 것이 n-gram\n",
    "\n",
    "n-gram은 연속적으로 2개의 단어들을 순차적으로 이동하면서 단어들을 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bag of Words - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 텍스트 분류 실습 - 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
